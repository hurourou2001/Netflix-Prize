{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "DZgaJTqRwVpJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgcKEj48DWMA",
        "outputId": "c568e7e5-2b27-43b3-de6e-587ba9673dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/netflix-inc/netflix-prize-data/versions/2\n",
            "Files in the dataset: ['probe.txt', 'combined_data_4.txt', 'qualifying.txt', 'README', 'combined_data_3.txt', 'movie_titles.csv', 'combined_data_2.txt', 'combined_data_1.txt']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import zipfile\n",
        "\n",
        "# Step 1: Download the dataset using kagglehub\n",
        "path = kagglehub.dataset_download(\"netflix-inc/netflix-prize-data\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Step 2: Verify the downloaded files\n",
        "files = os.listdir(path)\n",
        "print(\"Files in the dataset:\", files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths for all combined_data files\n",
        "file_paths = [\n",
        "    \"/root/.cache/kagglehub/datasets/netflix-inc/netflix-prize-data/versions/2/combined_data_1.txt\",\n",
        "    \"/root/.cache/kagglehub/datasets/netflix-inc/netflix-prize-data/versions/2/combined_data_2.txt\",\n",
        "    \"/root/.cache/kagglehub/datasets/netflix-inc/netflix-prize-data/versions/2/combined_data_3.txt\",\n",
        "    \"/root/.cache/kagglehub/datasets/netflix-inc/netflix-prize-data/versions/2/combined_data_4.txt\"\n",
        "]\n",
        "\n",
        "def process_single_file(file_path):\n",
        "    \"\"\"\n",
        "    Processes a single combined_data file and returns a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the combined_data file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the file's data.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    current_movie_id = None\n",
        "\n",
        "    # Read the file line by line\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()  # Remove extra whitespace\n",
        "            if line.endswith(':'):\n",
        "                # Movie ID line\n",
        "                current_movie_id = int(line[:-1])  # Remove ':' and convert to int\n",
        "            else:\n",
        "                # CustomerID, Rating, Date line\n",
        "                customer_id, rating, date = line.split(',')\n",
        "                rows.append([current_movie_id, int(customer_id), int(rating), date])\n",
        "\n",
        "    # Convert rows to a DataFrame\n",
        "    df = pd.DataFrame(rows, columns=['MovieID', 'CustomerID', 'Rating', 'Date'])\n",
        "    return df\n",
        "\n",
        "def combine_files(file_paths):\n",
        "    \"\"\"\n",
        "    Combines data from multiple combined_data files.\n",
        "\n",
        "    Args:\n",
        "        file_paths (list): List of file paths to combine.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A single combined DataFrame.\n",
        "    \"\"\"\n",
        "    data_frames = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        print(f\"Processing file: {file_path}\")\n",
        "        df = process_single_file(file_path)\n",
        "        data_frames.append(df)\n",
        "\n",
        "    # Concatenate all DataFrames\n",
        "    combined_data = pd.concat(data_frames, ignore_index=True)\n",
        "    return combined_data\n",
        "\n",
        "# Combine all four combined_data files\n",
        "combined_data = combine_files(file_paths)\n",
        "\n",
        "# Display basic information about the combined data\n",
        "print(\"Combined Data Overview:\")\n",
        "print(combined_data.info())\n",
        "print(combined_data.head())\n"
      ],
      "metadata": {
        "id": "AoTC-x_4vXq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b4bc550-1aa1-43b5-bec3-f3debccb34fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: /root/.cache/kagglehub/datasets/netflix-inc/netflix-prize-data/versions/2/combined_data_1.txt\n",
            "Processing file: /root/.cache/kagglehub/datasets/netflix-inc/netflix-prize-data/versions/2/combined_data_2.txt\n",
            "Processing file: /root/.cache/kagglehub/datasets/netflix-inc/netflix-prize-data/versions/2/combined_data_3.txt\n",
            "Processing file: /root/.cache/kagglehub/datasets/netflix-inc/netflix-prize-data/versions/2/combined_data_4.txt\n",
            "Combined Data Overview:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100480507 entries, 0 to 100480506\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Dtype \n",
            "---  ------      ----- \n",
            " 0   MovieID     int64 \n",
            " 1   CustomerID  int64 \n",
            " 2   Rating      int64 \n",
            " 3   Date        object\n",
            "dtypes: int64(3), object(1)\n",
            "memory usage: 3.0+ GB\n",
            "None\n",
            "   MovieID  CustomerID  Rating        Date\n",
            "0        1     1488844       3  2005-09-06\n",
            "1        1      822109       5  2005-05-13\n",
            "2        1      885013       4  2005-10-19\n",
            "3        1       30878       4  2005-12-26\n",
            "4        1      823519       3  2004-05-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Check if the directory exists\n",
        "directory = '/content/drive/My Drive/Colab Notebooks'\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)  # Create the directory if it doesn't exist\n",
        "    print(f\"Directory created: {directory}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {directory}\")\n",
        "\n",
        "\n",
        "\n",
        "# Save the combined data to a CSV file\n",
        "output_file = '/content/drive/My Drive/Colab Notebooks/combined_data_all.csv'\n",
        "# Combined data has columns: ['MovieID', 'CustomerID', 'Rating', 'Date']\n",
        "combined_data.to_csv(output_file, index=False)\n",
        "combined_data = pd.read_csv(output_file, names=[\"MovieID\", \"CustomerID\", \"Rating\", \"Date\"], header=0)\n",
        "\n",
        "print(f\"Combined data saved to {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcjHrVVepc7B",
        "outputId": "24bdce17-5ca8-4e11-d7e7-5822ec41b072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Directory already exists: /content/drive/My Drive/Colab Notebooks\n",
            "Combined data saved to /content/drive/My Drive/Colab Notebooks/combined_data_all.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Features Engineering\n",
        "\n",
        "\n",
        "\n",
        "*   The Data Features Engineering cell is corresponding to section 3 (Dataset and Features) in our paper.\n",
        "*   we have four parts in the following cell : preprocessing, features extracted, data splits, data storage.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V9zmwB7JwgOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import cupy as cp\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "from google.colab import drive\n",
        "from cupyx.scipy.sparse import csr_matrix\n",
        "from cupyx.scipy.sparse.linalg import svds\n",
        "\n",
        "# Mount Google Drive for loading and saving data files\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "# Path to the Netflix Prize dataset\n",
        "output_file = '/content/drive/My Drive/Colab Notebooks/combined_data_all.csv'\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "\n",
        "\n",
        "\n",
        "# 1.1 Normalize Ratings\n",
        "# Load the dataset with columns: MovieID, CustomerID, Rating, and Date\n",
        "combined_data = pd.read_csv(output_file, names=[\"MovieID\", \"CustomerID\", \"Rating\", \"Date\"], header=0, low_memory=False)\n",
        "# Ensure the Rating column is numeric; convert invalid entries to NaN\n",
        "combined_data['Rating'] = pd.to_numeric(combined_data['Rating'], errors='coerce')\n",
        "\n",
        "## error checking\n",
        "print(f\"Total rows in combined_data: {len(combined_data)}\")\n",
        "print(combined_data.head())\n",
        "print(combined_data.info())\n",
        "\n",
        "# Drop rows with missing or invalid ratings\n",
        "combined_data = combined_data.dropna(subset=['Rating'])\n",
        "\n",
        "# Convert the Rating column to a GPU array using CuPy for fast computation\n",
        "ratings_gpu = cp.array(combined_data['Rating'].values, dtype=cp.float32)\n",
        "\n",
        "# Compute the global mean rating using GPU acceleration\n",
        "global_mean = cp.mean(ratings_gpu).get()\n",
        "# Normalize ratings by subtracting the global mean to reduce user bias\n",
        "combined_data['NormalizedRating'] = combined_data['Rating'] - global_mean\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1.2 Filter Users and Movies\n",
        "# Calculate the number of ratings for each user and movie\n",
        "user_counts = combined_data['CustomerID'].value_counts()\n",
        "movie_counts = combined_data['MovieID'].value_counts()\n",
        "\n",
        "# Debugging information to check filtering thresholds\n",
        "print(f\"Number of users: {len(user_counts)}\")\n",
        "print(f\"Users with at least 10 ratings: {sum(user_counts >= 10)}\")\n",
        "print(f\"Number of movies: {len(movie_counts)}\")\n",
        "print(f\"Movies with at least 5 ratings: {sum(movie_counts >= 5)}\")\n",
        "\n",
        "# Identify low-activity users and low-rated movies\n",
        "low_activity_users = user_counts[user_counts < 10].index\n",
        "low_rated_movies = movie_counts[movie_counts < 5].index\n",
        "\n",
        "# Filter users with fewer than 10 ratings\n",
        "# All the movies except one has at least 10 ratings, so no further filter for movie\n",
        "filtered_data = combined_data[\n",
        "    combined_data['CustomerID'].isin(user_counts[user_counts >= 10].index)\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1.3 Transform Dates\n",
        "# Convert the Date column to a datetime format\n",
        "filtered_data['Date'] = pd.to_datetime(filtered_data['Date'])\n",
        "\n",
        "# Calculate the number of days since the earliest rating for each row\n",
        "filtered_data['DaysSinceFirstRating'] = (filtered_data['Date'] - filtered_data['Date'].min()).dt.days\n",
        "# Remove the original Date column after transformation\n",
        "filtered_data.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Feature Engineering\n",
        "# 2.1 Calculate User and Movie Mean Ratings\n",
        "# the User mean rating and movie mean rating are new features included\n",
        "# Compute the mean normalized rating for each user and movie\n",
        "user_avg_ratings = filtered_data.groupby('CustomerID')['NormalizedRating'].mean()\n",
        "movie_avg_ratings = filtered_data.groupby('MovieID')['NormalizedRating'].mean()\n",
        "# Map the mean ratings back to the dataset as new features\n",
        "filtered_data['UserMeanRating'] = filtered_data['CustomerID'].map(user_avg_ratings)\n",
        "filtered_data['MovieMeanRating'] = filtered_data['MovieID'].map(movie_avg_ratings)\n",
        "\n",
        "# Ensure filtered_data is not empty after preprocessing\n",
        "if filtered_data.empty:\n",
        "    raise ValueError(\"Filtered data is empty. Please check filtering conditions or input data.\")\n",
        "\n",
        "# Debugging information to confirm data size\n",
        "print(f\"Filtered data size: {filtered_data.shape[0]} rows\")\n",
        "\n",
        "# Ensure that the DaysSinceFirstRating column is populated\n",
        "if filtered_data[['DaysSinceFirstRating']].shape[0] == 0:\n",
        "    raise ValueError(\"No data available for temporal bucketing.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2.2 Temporal Bucketing\n",
        "# Discretize DaysSinceFirstRating into 10 equal-width buckets for temporal analysis\n",
        "discret = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
        "\n",
        "filtered_data['DaysSinceFirstRatingBucket'] = discret.fit_transform(\n",
        "    filtered_data[['DaysSinceFirstRating']]\n",
        ").astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Splitting the Data\n",
        "# Training: 80%, Validation: 10%, Testing: 10%\n",
        "train_data, temp_data = train_test_split(filtered_data, test_size=0.2, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 4: Matrix Conversion\n",
        "# Encode users and movies into numerical indices for efficient matrix operations\n",
        "train_data['UserEncoded'] = train_data['CustomerID'].astype('category').cat.codes\n",
        "train_data['MovieEncoded'] = train_data['MovieID'].astype('category').cat.codes\n",
        "\n",
        "# Apply same encoding for validation and test sets\n",
        "val_data['UserEncoded'] = val_data['CustomerID'].astype('category').cat.codes\n",
        "val_data['MovieEncoded'] = val_data['MovieID'].astype('category').cat.codes\n",
        "test_data['UserEncoded'] = test_data['CustomerID'].astype('category').cat.codes\n",
        "test_data['MovieEncoded'] = test_data['MovieID'].astype('category').cat.codes\n",
        "\n",
        "\n",
        "# 4.1 User-Movie Interaction Matrix\n",
        "# Convert data into arrays for matrix factorization\n",
        "# Convert data to CuPy arrays\n",
        "customer_ids = cp.array(filtered_data['CustomerID'].astype('category').cat.codes)\n",
        "movie_ids = cp.array(filtered_data['MovieID'].astype('category').cat.codes)\n",
        "normalized_ratings = cp.array(filtered_data['NormalizedRating'])\n",
        "\n",
        "# Create a sparse interaction matrix\n",
        "# Convert to scalar integers using .item()\n",
        "num_customers = customer_ids.max().item() + 1\n",
        "num_movies = movie_ids.max().item() + 1\n",
        "\n",
        "# Create a sparse matrix for interactions\n",
        "interaction_matrix_gpu = csr_matrix((normalized_ratings, (customer_ids, movie_ids)),\n",
        "                                    shape=(customer_ids.max() + 1, movie_ids.max() + 1))\n",
        "\n",
        "\n",
        "\n",
        "# 4.2 Perform SVD (Singular Value Decomposition)\n",
        "# Decompose the interaction matrix into latent features for users and movies\n",
        "# Perform SVD with CuPy\n",
        "u, s, vt = svds(interaction_matrix_gpu, k=2)  # Top 4 latent features\n",
        "\n",
        "# Convert latent features to NumPy arrays for further processing\n",
        "latent_features = cp.asnumpy(u)  # Use u for user latent features\n",
        "latent_features_df = pd.DataFrame(latent_features)# Create DataFrames for latent features\n",
        "latent_features_df['CustomerID'] = filtered_data['CustomerID'].astype('category').cat.categories\n",
        "\n",
        "movie_latent_features = cp.asnumpy(vt.T)  # Use vt for Movie latent features\n",
        "movie_latent_features_df = pd.DataFrame(movie_latent_features)\n",
        "movie_latent_features_df['MovieID'] = filtered_data['MovieID'].astype('category').cat.categories\n",
        "\n",
        "# Reset the index for merging\n",
        "latent_features_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Merge latent features back into training, validation, and testing datasets\n",
        "train_data = train_data.merge(latent_features_df, on='CustomerID', how='left')\n",
        "val_data = val_data.merge(latent_features_df, on='CustomerID', how='left')\n",
        "test_data = test_data.merge(latent_features_df, on='CustomerID', how='left')\n",
        "train_data = train_data.merge(movie_latent_features_df, on='MovieID', how='left')\n",
        "val_data = val_data.merge(movie_latent_features_df, on='MovieID', how='left')\n",
        "test_data = test_data.merge(movie_latent_features_df, on='MovieID', how='left')\n",
        "\n",
        "# Fill missing latent feature values with 0\n",
        "train_data.fillna(0, inplace=True)\n",
        "val_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "# Reapply UserEncoded and MovieEncoded after merging\n",
        "train_data['UserEncoded'] = train_data['CustomerID'].astype('category').cat.codes\n",
        "train_data['MovieEncoded'] = train_data['MovieID'].astype('category').cat.codes\n",
        "\n",
        "val_data['UserEncoded'] = val_data['CustomerID'].astype('category').cat.codes\n",
        "val_data['MovieEncoded'] = val_data['MovieID'].astype('category').cat.codes\n",
        "\n",
        "test_data['UserEncoded'] = test_data['CustomerID'].astype('category').cat.codes\n",
        "test_data['MovieEncoded'] = test_data['MovieID'].astype('category').cat.codes\n",
        "\n",
        "# Define latent feature columns\n",
        "\"\"\"\n",
        "Index(['MovieID', 'CustomerID', 'Rating', 'NormalizedRating',\n",
        "       'DaysSinceFirstRating', 'UserMeanRating', 'MovieMeanRating',\n",
        "       'DaysSinceFirstRatingBucket', '0_x', '1_x', '0_y', '1_y', 'UserEncoded',\n",
        "       'MovieEncoded'],\n",
        "      dtype='object')\n",
        "train data columns above\n",
        "\"\"\"\n",
        "latent_feature_cols = ['0_x', '1_x', '0_y', '1_y']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 5: Define Features and Targets\n",
        "X_train = train_data[['UserEncoded', 'MovieEncoded', 'DaysSinceFirstRating',\n",
        "                      'UserMeanRating', 'MovieMeanRating', 'DaysSinceFirstRatingBucket'] + latent_feature_cols]\n",
        "X_val = val_data[['UserEncoded', 'MovieEncoded', 'DaysSinceFirstRating',\n",
        "                  'UserMeanRating', 'MovieMeanRating', 'DaysSinceFirstRatingBucket'] + latent_feature_cols]\n",
        "X_test = test_data[['UserEncoded', 'MovieEncoded', 'DaysSinceFirstRating',\n",
        "                    'UserMeanRating', 'MovieMeanRating', 'DaysSinceFirstRatingBucket'] + latent_feature_cols]\n",
        "# 5.1: Create interaction terms\n",
        "# we create the interaction terms because their might exist internal relationship between latent_features.\n",
        "for i, user_col in enumerate(latent_feature_cols[:2]):  # User latent features are the first two columns\n",
        "    for j, movie_col in enumerate(latent_feature_cols[2:]):  # Movie latent features are the last two columns\n",
        "        X_train[f'interaction_{i}_{j}'] = X_train[user_col] * X_train[movie_col]\n",
        "        X_val[f'interaction_{i}_{j}'] = X_val[user_col] * X_val[movie_col]\n",
        "        X_test[f'interaction_{i}_{j}'] = X_test[user_col] * X_test[movie_col]\n",
        "\n",
        "\n",
        "# Extract the response variables.\n",
        "y_train = train_data['NormalizedRating']\n",
        "y_val = val_data['NormalizedRating']\n",
        "y_test = test_data['NormalizedRating']\n",
        "\n",
        "# Fill missing values with 0\n",
        "X_train.fillna(0, inplace=True)\n",
        "X_val.fillna(0, inplace=True)\n",
        "X_test.fillna(0, inplace=True)\n",
        "\n",
        "# Since each time processing data feature engineering requires at least an hour,\n",
        "# we directly store the data and no longer need to redo the above data feature engineering part once offline\n",
        "# Here, it need google drive at least 100 GB storage\n",
        "X_train.to_csv('/content/drive/My Drive/Colab Notebooks/X_train.csv', index=False)\n",
        "X_val.to_csv('/content/drive/My Drive/Colab Notebooks/X_val.csv', index=False)\n",
        "X_test.to_csv('/content/drive/My Drive/Colab Notebooks/X_test.csv', index=False)\n",
        "\n",
        "y_train.to_csv('/content/drive/My Drive/Colab Notebooks/y_train.csv', index=False)\n",
        "y_val.to_csv('/content/drive/My Drive/Colab Notebooks/y_val.csv', index=False)\n",
        "y_test.to_csv('/content/drive/My Drive/Colab Notebooks/y_test.csv', index=False)\n",
        "print(\"Datasets saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByUHU_7ew5kU",
        "outputId": "829c1e2c-6ea9-46c3-e498-001c087e0463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Total rows in combined_data: 100480507\n",
            "   MovieID  CustomerID  Rating        Date\n",
            "0        1     1488844       3  2005-09-06\n",
            "1        1      822109       5  2005-05-13\n",
            "2        1      885013       4  2005-10-19\n",
            "3        1       30878       4  2005-12-26\n",
            "4        1      823519       3  2004-05-03\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100480507 entries, 0 to 100480506\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Dtype \n",
            "---  ------      ----- \n",
            " 0   MovieID     int64 \n",
            " 1   CustomerID  int64 \n",
            " 2   Rating      int64 \n",
            " 3   Date        object\n",
            "dtypes: int64(3), object(1)\n",
            "memory usage: 3.0+ GB\n",
            "None\n",
            "Number of users: 480189\n",
            "Users with at least 10 ratings: 463770\n",
            "Number of movies: 17770\n",
            "Movies with at least 5 ratings: 17769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ea832b8c194a>:65: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_data['Date'] = pd.to_datetime(filtered_data['Date'])\n",
            "<ipython-input-3-ea832b8c194a>:66: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_data['DaysSinceFirstRating'] = (filtered_data['Date'] - filtered_data['Date'].min()).dt.days\n",
            "<ipython-input-3-ea832b8c194a>:67: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_data.drop(columns=['Date'], inplace=True)\n",
            "<ipython-input-3-ea832b8c194a>:74: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_data['UserMeanRating'] = filtered_data['CustomerID'].map(user_mean_ratings)\n",
            "<ipython-input-3-ea832b8c194a>:75: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_data['MovieMeanRating'] = filtered_data['MovieID'].map(movie_mean_ratings)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered data size: 100396384 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ea832b8c194a>:92: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_data['DaysSinceFirstRatingBucket'] = discretizer.fit_transform(\n",
            "<ipython-input-3-ea832b8c194a>:194: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train[f'interaction_{i}_{j}'] = X_train[user_col] * X_train[movie_col]\n",
            "<ipython-input-3-ea832b8c194a>:195: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_val[f'interaction_{i}_{j}'] = X_val[user_col] * X_val[movie_col]\n",
            "<ipython-input-3-ea832b8c194a>:196: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[f'interaction_{i}_{j}'] = X_test[user_col] * X_test[movie_col]\n",
            "<ipython-input-3-ea832b8c194a>:194: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train[f'interaction_{i}_{j}'] = X_train[user_col] * X_train[movie_col]\n",
            "<ipython-input-3-ea832b8c194a>:195: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_val[f'interaction_{i}_{j}'] = X_val[user_col] * X_val[movie_col]\n",
            "<ipython-input-3-ea832b8c194a>:196: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[f'interaction_{i}_{j}'] = X_test[user_col] * X_test[movie_col]\n",
            "<ipython-input-3-ea832b8c194a>:194: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train[f'interaction_{i}_{j}'] = X_train[user_col] * X_train[movie_col]\n",
            "<ipython-input-3-ea832b8c194a>:195: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_val[f'interaction_{i}_{j}'] = X_val[user_col] * X_val[movie_col]\n",
            "<ipython-input-3-ea832b8c194a>:196: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[f'interaction_{i}_{j}'] = X_test[user_col] * X_test[movie_col]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the partition data."
      ],
      "metadata": {
        "id": "FYNW_3BbGG1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets from Google Drive if you are offline.\n",
        "# No longer need to do the feature engineering part again.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import cupy as cp\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "from google.colab import drive\n",
        "from cupyx.scipy.sparse import csr_matrix\n",
        "from cupyx.scipy.sparse.linalg import svds\n",
        "# Reload the data back once offline.\n",
        "X_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/X_train.csv')\n",
        "X_val = pd.read_csv('/content/drive/My Drive/Colab Notebooks/X_val.csv')\n",
        "X_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/X_test.csv')\n",
        "\n",
        "y_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/y_train.csv')\n",
        "y_val = pd.read_csv('/content/drive/My Drive/Colab Notebooks/y_val.csv')\n",
        "y_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/y_test.csv')\n",
        "\n",
        "print(\"Datasets loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nukdOdrGFkF",
        "outputId": "8b05ef79-ae59-4d75-8672-2205044108a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Engineering\n",
        "\n",
        "\n",
        "This project explores methods to predict Netflix movie ratings. We employ the following models:\n",
        "\n",
        "1. **Linear Regression with SVD Features**\n",
        "    - Utilizes Singular Value Decomposition (SVD) to extract latent features from the user-movie interaction matrix.\n",
        "\n",
        "2. **Ridge Regression**\n",
        "    - Adds regularization to mitigate overfitting in rating predictions.\n",
        "\n",
        "3. **Temporal Matrix Factorization (TMF)**\n",
        "    - Captures temporal dynamics by modeling user and movie preferences over time.\n",
        "\n",
        "4. **XGBoost**\n",
        "    - Leverages gradient-boosted decision trees for flexible and robust predictive performance.\n",
        "\n",
        "### Cold-Start Problem with BPR\n",
        "\n",
        "To address the cold-start problem, we integrate **Bayesian Personalized Ranking (BPR)**. This approach evaluates how effectively the system can recommend the top 10 highest-rated movies for new users entering the platform.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R1ksN5Kow03G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression + SVD Features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wgkc-rA6R4VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# 4.2.1 Linear regression\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train, y_train)\n",
        "# Step 1: Training Accuracy\n",
        "y_train_pred = reg.predict(X_train)\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "print(\"Experiement result for Linear Regression + SVD Features\")\n",
        "print(\"Training RMSE:\", train_rmse)\n",
        "\n",
        "# Step 2: Validation Accuracy\n",
        "y_val_pred = reg.predict(X_val)\n",
        "val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "val_rmse = np.sqrt(val_mse)\n",
        "print(\"Validation RMSE:\", val_rmse)\n",
        "\n",
        "# Step 3: Test Accuracy\n",
        "y_test_pred = reg.predict(X_test)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "print(\"Test RMSE:\", test_rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cY4vQrpw6RP",
        "outputId": "b7fb2853-31bf-4dc6-f614-f689e3dc0e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiement result for Linear Regression + SVD Features\n",
            "Training RMSE: 0.9051700746569552\n",
            "Validation RMSE: 0.9048902594688573\n",
            "Test RMSE: 0.9052695350111042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge Regression Implementation\n"
      ],
      "metadata": {
        "id": "qkTi_NIUfMYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "# 4.2.2 Ridge regression.\n",
        "# Define a range of alpha values to test\n",
        "alpha_values = [0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "# Store results for analysis\n",
        "results = []\n",
        "\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    # Initialize Ridge Regressor with the current alpha\n",
        "    ridge_regressor = Ridge(alpha=alpha)\n",
        "\n",
        "    # Train the Ridge model\n",
        "    ridge_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = ridge_regressor.predict(X_train)\n",
        "    y_val_pred = ridge_regressor.predict(X_val)\n",
        "    y_test_pred = ridge_regressor.predict(X_test)\n",
        "\n",
        "    # Compute RMSE\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    # Append results\n",
        "    results.append({\n",
        "        'alpha': alpha,\n",
        "        'train_rmse': train_rmse,\n",
        "        'val_rmse': val_rmse,\n",
        "        'test_rmse': test_rmse,\n",
        "\n",
        "    })\n",
        "    print(f\"Alpha: {alpha}, Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
        "\n",
        "# Find the best alpha based on validation RMSE\n",
        "best_result = min(results, key=lambda x: x['val_rmse'])\n",
        "print(\"\\nBest Alpha and Results:\")\n",
        "print(f\"Alpha: {best_result['alpha']}\")\n",
        "print(f\"Train RMSE: {best_result['train_rmse']:.4f}, Val RMSE: {best_result['val_rmse']:.4f}, Test RMSE: {best_result['test_rmse']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynaOU0kOfLVy",
        "outputId": "b6b408d6-fc6b-4438-f015-9309ec4aa6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.10611e-20): result may not be accurate.\n",
            "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 0.01, Train RMSE: 0.9179, Val RMSE: 0.9177, Test RMSE: 0.9180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.09119e-19): result may not be accurate.\n",
            "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 0.1, Train RMSE: 0.9185, Val RMSE: 0.9183, Test RMSE: 0.9186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.00386e-19): result may not be accurate.\n",
            "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 1, Train RMSE: 0.9215, Val RMSE: 0.9212, Test RMSE: 0.9216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.82671e-18): result may not be accurate.\n",
            "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 10, Train RMSE: 0.9229, Val RMSE: 0.9226, Test RMSE: 0.9230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.85961e-17): result may not be accurate.\n",
            "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 100, Train RMSE: 0.9231, Val RMSE: 0.9229, Test RMSE: 0.9232\n",
            "\n",
            "Best Alpha and Results:\n",
            "Alpha: 0.01\n",
            "Train RMSE: 0.9179, Val RMSE: 0.9177, Test RMSE: 0.9180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Factorization with Temporal Dynamics"
      ],
      "metadata": {
        "id": "Ey32KtzEw1xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# 4.2.3 Temporal Matrix Factorization (TMF)\n",
        "# Function to train a Temporal Matrix Factorization model with batch processing\n",
        "def TMF(data, num_factors, learning_rate, num_epochs, lambda_reg, batch_size, patience=2):\n",
        "  \"\"\"\n",
        "    Trains a Temporal Matrix Factorization model using SGD with batch processing.\n",
        "\n",
        "    Parameters:\n",
        "        data: DataFrame containing training data with 'UserEncoded', 'MovieEncoded', 'DaysSinceFirstRatingBucket', and 'NormalizedRating'.\n",
        "        num_factors: Number of latent factors for users and movies.\n",
        "        learning_rate: Learning rate for gradient updates.\n",
        "        num_epochs: Maximum number of training epochs.\n",
        "        lambda_reg: Regularization term to prevent overfitting.\n",
        "        batch_size: Number of samples per training batch.\n",
        "        patience: Number of epochs to wait for improvement before early stopping.\n",
        "\n",
        "    Returns:\n",
        "        user_latent_matrix, movie_latent_matrix: Learned user and movie latent factor matrices.\n",
        "        user_time_bias, movie_time_bias: Learned user and movie time biases.\n",
        "    \"\"\"\n",
        "    # Initialization of parameters\n",
        "    num_users = data['UserEncoded'].max() + 1\n",
        "    num_movies = data['MovieEncoded'].max() + 1\n",
        "    num_buckets = data['DaysSinceFirstRatingBucket'].max() + 1\n",
        "\n",
        "    # Random initialization of user and movie latent factor matrices\n",
        "    user_latent_matrix = cp.random.normal(scale=0.1, size=(num_users, num_factors), dtype=cp.float32)\n",
        "    movie_latent_matrix = cp.random.normal(scale=0.1, size=(num_movies, num_factors), dtype=cp.float32)\n",
        "    # Initialize time biases for users and movies\n",
        "    user_time_bias = cp.zeros((num_users, num_buckets), dtype=cp.float32)\n",
        "    movie_time_bias = cp.zeros((num_movies, num_buckets), dtype=cp.float32)\n",
        "    # Calculate global mean rating\n",
        "    global_mean = y_train['NormalizedRating'].mean()\n",
        "\n",
        "    # Convert data columns to GPU arrays for computation\n",
        "    user_idx = cp.array(data['UserEncoded'].values)\n",
        "    movie_idx = cp.array(data['MovieEncoded'].values)\n",
        "    bucket_idx = cp.array(data['DaysSinceFirstRatingBucket'].values)\n",
        "    ratings = cp.array(y_train['NormalizedRating'].values)\n",
        "\n",
        "    # Variables to track best loss for early stopping\n",
        "    best_loss = float('inf')\n",
        "    early_stop_count = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "      # Shuffle the data for each epoch\n",
        "        shuffled_indices = cp.random.permutation(len(data))\n",
        "        user_idx = user_idx[shuffled_indices]\n",
        "        movie_idx = movie_idx[shuffled_indices]\n",
        "        bucket_idx = bucket_idx[shuffled_indices]\n",
        "        ratings = ratings[shuffled_indices]\n",
        "\n",
        "        epoch_loss = 0 # Accumulator for loss during the epoch\n",
        "\n",
        "        # Process data in batches\n",
        "        for start in range(0, len(data), batch_size):\n",
        "            end = start + batch_size\n",
        "            batch_user_idx = user_idx[start:end]\n",
        "            batch_movie_idx = movie_idx[start:end]\n",
        "            batch_bucket_idx = bucket_idx[start:end]\n",
        "            batch_ratings = ratings[start:end]\n",
        "\n",
        "            # Compute predicted ratings using current parameters\n",
        "            user_latents = user_latent_matrix[batch_user_idx]\n",
        "            movie_latents = movie_latent_matrix[batch_movie_idx]\n",
        "            predicted_ratings = global_mean + \\\n",
        "                user_time_bias[batch_user_idx, batch_bucket_idx] + \\\n",
        "                movie_time_bias[batch_movie_idx, batch_bucket_idx] + \\\n",
        "                cp.einsum('ij,ij->i', user_latents, movie_latents)\n",
        "\n",
        "            # Compute prediction errors\n",
        "            errors = batch_ratings - predicted_ratings\n",
        "\n",
        "            # Update biases for users and movies\n",
        "            cp.add.at(user_time_bias, (batch_user_idx, batch_bucket_idx),\n",
        "                      learning_rate * (errors - lambda_reg * user_time_bias[batch_user_idx, batch_bucket_idx]))\n",
        "            cp.add.at(movie_time_bias, (batch_movie_idx, batch_bucket_idx),\n",
        "                      learning_rate * (errors - lambda_reg * movie_time_bias[batch_movie_idx, batch_bucket_idx]))\n",
        "\n",
        "            # Compute gradients for user and movie latent factors\n",
        "            grad_user = learning_rate * (errors[:, None] * movie_latents - lambda_reg * user_latents)\n",
        "            grad_movie = learning_rate * (errors[:, None] * user_latents - lambda_reg * movie_latents)\n",
        "\n",
        "            # Gradient clipping to avoid exploding gradients\n",
        "            cp.clip(grad_user, -2, 2, out=grad_user)\n",
        "            cp.clip(grad_movie, -2, 2, out=grad_movie)\n",
        "\n",
        "            # Update latent factors\n",
        "            user_latent_matrix[batch_user_idx] += grad_user\n",
        "            movie_latent_matrix[batch_movie_idx] += grad_movie\n",
        "\n",
        "            # Accumulate batch loss\n",
        "            batch_loss = cp.mean(errors**2)\n",
        "            epoch_loss += batch_loss\n",
        "\n",
        "        # Compute epoch loss and add regularization penalty\n",
        "        epoch_loss = epoch_loss / (len(data) // batch_size)\n",
        "        epoch_loss += lambda_reg * (\n",
        "            cp.linalg.norm(user_latent_matrix)**2 + cp.linalg.norm(movie_latent_matrix)**2\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss.get():.4f}\")\n",
        "\n",
        "        # Early stopping logic\n",
        "        # oour patience is 2, once the training loss does not decrease for more than 2 times\n",
        "        # we stop\n",
        "        if epoch_loss.get() < best_loss:\n",
        "            best_loss = epoch_loss.get()\n",
        "            early_stop_count = 0\n",
        "            # Save the best parameters for later testing and validation\n",
        "            best_user_latent_matrix = user_latent_matrix.copy()\n",
        "            best_movie_latent_matrix = movie_latent_matrix.copy()\n",
        "            best_user_time_bias = user_time_bias.copy()\n",
        "            best_movie_time_bias = movie_time_bias.copy()\n",
        "        else:\n",
        "            early_stop_count += 1\n",
        "            if early_stop_count >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "    print(f\"Best Loss: {best_loss:.4f}\")\n",
        "    return user_latent_matrix, movie_latent_matrix, user_time_bias, movie_time_bias\n",
        "\n",
        "\n",
        "# Function to evaluate the trained model\n",
        "def evaluate_model(data, y_normal, user_latent_matrix, movie_latent_matrix, user_time_bias, movie_time_bias, global_mean):\n",
        "  \"\"\"\n",
        "    Evaluates the Temporal Matrix Factorization model using RMSE.\n",
        "\n",
        "    Parameters:\n",
        "        data: DataFrame containing evaluation data.\n",
        "        y_normal: DataFrame with normalized ratings.\n",
        "        user_latent_matrix, movie_latent_matrix: Latent factor matrices from training.\n",
        "        user_time_bias, movie_time_bias: Time biases from training.\n",
        "        global_mean: Mean of the training ratings.\n",
        "\n",
        "    Returns:\n",
        "        RMSE value for the given dataset.\n",
        "    \"\"\"\n",
        "    # Convert data columns to GPU arrays for evaluation\n",
        "    user_idx = cp.array(data['UserEncoded'].values)\n",
        "    movie_idx = cp.array(data['MovieEncoded'].values)\n",
        "    bucket_idx = cp.array(data['DaysSinceFirstRatingBucket'].values)\n",
        "    ratings = cp.array(y_normal['NormalizedRating'].values)\n",
        "\n",
        "     # Compute predicted ratings\n",
        "    user_latents = user_latent_matrix[user_idx]\n",
        "    movie_latents = movie_latent_matrix[movie_idx]\n",
        "    predicted_ratings = global_mean + \\\n",
        "        user_time_bias[user_idx, bucket_idx] + \\\n",
        "        movie_time_bias[movie_idx, bucket_idx] + \\\n",
        "        cp.einsum('ij,ij->i', user_latents, movie_latents)\n",
        "\n",
        "    # Compute RMSE\n",
        "    mse = mean_squared_error(cp.asnumpy(ratings), cp.asnumpy(predicted_ratings))\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    return rmse\n",
        "\n",
        "# Hyper parameters tunning\n",
        "num_factors = 15\n",
        "learning_rate = 0.1\n",
        "num_epochs = 20\n",
        "lambda_reg = 0.3\n",
        "batch_size = 1024\n",
        "patience = 2  # Early stopping patience\n",
        "\n",
        "train_data = X_train\n",
        "val_data = X_val\n",
        "test_data = X_test\n",
        "y_train_normal = y_train\n",
        "y_val_normal = y_val\n",
        "y_test_normal = y_test\n",
        "\n",
        "\n",
        "user_latent_matrix, movie_latent_matrix, user_time_bias, movie_time_bias = TMF(\n",
        "    train_data, num_factors, learning_rate, num_epochs, lambda_reg, batch_size, patience\n",
        ")\n",
        "\n",
        "global_mean = y_train['NormalizedRating'].mean()\n",
        "print(\"Training Set Evaluation:\")\n",
        "train_rmse = evaluate_model(train_data, y_train_normal, user_latent_matrix, movie_latent_matrix, user_time_bias, movie_time_bias, global_mean)\n",
        "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
        "# Evaluate on Validation and Test Sets\n",
        "print(\"Validation Set Evaluation:\")\n",
        "val_rmse = evaluate_model(val_data, y_val_normal, user_latent_matrix, movie_latent_matrix, user_time_bias, movie_time_bias, global_mean)\n",
        "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "print(\"Test Set Evaluation:\")\n",
        "test_rmse = evaluate_model(test_data, y_test_normal, user_latent_matrix, movie_latent_matrix, user_time_bias, movie_time_bias, global_mean)\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AffJh6tiw1go",
        "outputId": "b687e29a-e466-4d79-a5a4-0517cc3eb0cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1936.8648\n",
            "Epoch 2/20, Loss: 643.7501\n",
            "Epoch 3/20, Loss: 269.7832\n",
            "Epoch 4/20, Loss: 127.1088\n",
            "Epoch 5/20, Loss: 64.5522\n",
            "Epoch 6/20, Loss: 34.6731\n",
            "Epoch 7/20, Loss: 19.5223\n",
            "Epoch 8/20, Loss: 11.4945\n",
            "Epoch 9/20, Loss: 7.0891\n",
            "Epoch 10/20, Loss: 4.5997\n",
            "Epoch 11/20, Loss: 3.1587\n",
            "Epoch 12/20, Loss: 2.3058\n",
            "Epoch 13/20, Loss: 1.7915\n",
            "Epoch 14/20, Loss: 1.4756\n",
            "Epoch 15/20, Loss: 1.2787\n",
            "Epoch 16/20, Loss: 1.1537\n",
            "Epoch 17/20, Loss: 1.0733\n",
            "Epoch 18/20, Loss: 1.0208\n",
            "Epoch 19/20, Loss: 0.9862\n",
            "Epoch 20/20, Loss: 0.9628\n",
            "Best Loss: 0.9628\n",
            "Training Set Evaluation:\n",
            "RMSE: 0.9438\n",
            "Train RMSE: 0.9438\n",
            "Validation Set Evaluation:\n",
            "RMSE: 1.1763\n",
            "Validation RMSE: 1.1763\n",
            "Test Set Evaluation:\n",
            "RMSE: 1.1883\n",
            "Test RMSE: 1.1883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loss values for each epoch\n",
        "epochs = list(range(1, 21))\n",
        "loss_values = [\n",
        "    1936.8648, 643.7501, 269.7832, 127.1088, 64.5522, 34.6731, 19.5223,\n",
        "    11.4945, 7.0891, 4.5997, 3.1587, 2.3058, 1.7915, 1.4756, 1.2787,\n",
        "    1.1537, 1.0733, 1.0208, 0.9862, 0.9628\n",
        "]\n",
        "\n",
        "# Plot the loss graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, loss_values, marker='o', linestyle='-', color='b')\n",
        "plt.title('Training Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.xticks(epochs)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "uf2ukhRfl3BK",
        "outputId": "ff5b8820-d0e0-435c-feff-f9a2f45493a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPR Bayesian Personalized Ranking\n",
        "\n",
        "*  We add the BPR model becuase we mention it in draft report's introduction, but does not actually implement in draft report.\n",
        "\n",
        "*   Noting that the BPR problem is seperate with the above models, it only handle for the cold start problem\n",
        "\n",
        "*   the result in last cell mean if we choose 10 movies for the new user(cold - start), the system accuracy to recommend 10 high rating movie is 0.8345"
      ],
      "metadata": {
        "id": "mWsnKwmcWhHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Redo the feature extraction and Engineering"
      ],
      "metadata": {
        "id": "kZ1p2VPWkusX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oCKxzSGDqqKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import cupy as cp\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "from google.colab import drive\n",
        "from cupyx.scipy.sparse import csr_matrix\n",
        "from cupyx.scipy.sparse.linalg import svds\n",
        "\n",
        "# Load the dataset if offline\n",
        "# Here we do not need any further interaction features as well as matrix factorization feature\n",
        "# In that case, we re-do the data feature part, this would help our training process for BPR quicker\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "output_file = '/content/drive/My Drive/Colab Notebooks/combined_data_all.csv'\n",
        "combined_data = pd.read_csv(output_file, names=[\"MovieID\", \"CustomerID\", \"Rating\", \"Date\"], header=0, low_memory=False)\n",
        "\n",
        "# Ensure numeric ratings\n",
        "combined_data['Rating'] = pd.to_numeric(combined_data['Rating'], errors='coerce')\n",
        "combined_data.dropna(subset=['Rating'], inplace=True)\n",
        "\n",
        "# Filter active users and movies\n",
        "user_counts = combined_data['CustomerID'].value_counts()\n",
        "movie_counts = combined_data['MovieID'].value_counts()\n",
        "\n",
        "filtered_data = combined_data[\n",
        "    combined_data['CustomerID'].isin(user_counts[user_counts >= 10].index)\n",
        "]\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "train_data_bpr, temp_data = train_test_split(filtered_data, test_size=0.2, random_state=42)\n",
        "val_data_bpr, test_data_bpr = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save processed datasets\n",
        "# Easy to use if offline\n",
        "train_data_bpr.to_csv('/content/drive/My Drive/Colab Notebooks/train_data_simple_bpr.csv', index=False)\n",
        "val_data_bpr.to_csv('/content/drive/My Drive/Colab Notebooks/val_data_simple_bpr.csv', index=False)\n",
        "test_data_bpr.to_csv('/content/drive/My Drive/Colab Notebooks/test_data_simple_bpr.csv', index=False)\n",
        "\n",
        "print(\"Feature engineering completed. Data saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuFrSGHVk1uS",
        "outputId": "740cc30b-d751-40ff-b41d-9846e9878246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Feature engineering completed. Data saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import cupy as cp\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "from google.colab import drive\n",
        "from cupyx.scipy.sparse import csr_matrix\n",
        "from cupyx.scipy.sparse.linalg import svds\n",
        "# If you are offline for the project but want to do the BPR testing, directly run this cell to recover data\n",
        "# Mount Google Drive to access data files stored in the user's Drive account\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Path to the combined dataset file\n",
        "output_file = '/content/drive/My Drive/Colab Notebooks/combined_data_all.csv'\n",
        "\n",
        "# Load the combined dataset\n",
        "# Dataset contains columns: MovieID, CustomerID, Rating, and Date\n",
        "combined_data = pd.read_csv(output_file, names=[\"MovieID\", \"CustomerID\", \"Rating\", \"Date\"],\n",
        "                             header=0, low_memory=False)\n",
        "\n",
        "# Reload preprocessed training, validation, and test datasets with raw ratings\n",
        "train_data_bpr = pd.read_csv('/content/drive/My Drive/Colab Notebooks/train_data_simple_bpr.csv')\n",
        "val_data_bpr = pd.read_csv('/content/drive/My Drive/Colab Notebooks/val_data_simple_bpr.csv')\n",
        "test_data_bpr = pd.read_csv('/content/drive/My Drive/Colab Notebooks/test_data_simple_bpr.csv')\n",
        "\n",
        "# Calculate thresholds for positive and negative ratings\n",
        "# using the thresholds is better than arbitry choose the rating number for good or bad movie\n",
        "# These thresholds are derived based on the 75th and 25th percentiles of the ratings\n",
        "rating_75th = combined_data['Rating'].quantile(0.75)  # Threshold for positive interactions\n",
        "rating_25th = combined_data['Rating'].quantile(0.25)  # Threshold for negative interactions\n",
        "\n",
        "# Print thresholds for debugging and verification\n",
        "print(f\"Positive Threshold: >{rating_75th}\")  # Ratings above this are considered positive\n",
        "print(f\"Negative Threshold: <{rating_25th}\")  # Ratings below this are considered negative\n",
        "\n",
        "\n",
        "# Function to generate interaction labels (positive, negative, neutral) based on thresholds\n",
        "def generate_interaction_labels(data, pos_threshold, neg_threshold):\n",
        "    \"\"\"\n",
        "    Generate interaction labels for the given dataset based on rating thresholds.\n",
        "\n",
        "    Parameters:\n",
        "        data (DataFrame): The input dataset containing a 'Rating' column.\n",
        "        pos_threshold (float): The minimum rating value to classify as positive interaction.\n",
        "        neg_threshold (float): The maximum rating value to classify as negative interaction.\n",
        "\n",
        "    Returns:\n",
        "        Series: Interaction labels (1 for positive, -1 for negative, 0 for neutral).\n",
        "    \"\"\"\n",
        "    # Initialize all interactions as neutral (0)\n",
        "    interactions = pd.Series(0, index=data.index)\n",
        "\n",
        "    # Assign positive interaction label (1) for ratings above the positive threshold\n",
        "    interactions[data['Rating'] > pos_threshold] = 1\n",
        "\n",
        "    # Assign negative interaction label (-1) for ratings below the negative threshold\n",
        "    interactions[data['Rating'] < neg_threshold] = -1\n",
        "\n",
        "    return interactions\n",
        "\n",
        "# Generate interaction labels for the training, validation, and test datasets\n",
        "# Using the calculated thresholds for positive and negative interactions\n",
        "y_train_interactions = generate_interaction_labels(train_data_bpr, rating_75th, rating_25th)\n",
        "y_val_interactions = generate_interaction_labels(val_data_bpr, rating_75th, rating_25th)\n",
        "y_test_interactions = generate_interaction_labels(test_data_bpr, rating_75th, rating_25th)\n",
        "\n",
        "# At this point:\n",
        "# y_train_interactions, y_val_interactions, and y_test_interactions contain interaction labels\n",
        "# These labels will be used for generating BPR (Bayesian Personalized Ranking) training pairs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dUFx_OZNA5kw",
        "outputId": "5ab91436-4f98-4f6d-acc0-08804c4500b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Positive Threshold: >4.0\n",
            "Negative Threshold: <3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Define BCE Loss\n",
        "# Binary Cross Entropy with Logits Loss for training the BPR model\n",
        "crt = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Create mappings for zero-based indexing\n",
        "def Mapping(train_data):\n",
        "  \"\"\"\n",
        "    Create mappings for users and movies to unique zero-based indices.\n",
        "    These mappings are essential for encoding categorical IDs into numerical indices for embeddings.\n",
        "\n",
        "    Args:\n",
        "        train_data (DataFrame): Training dataset containing CustomerID and MovieID.\n",
        "\n",
        "    Returns:\n",
        "        user_mapping (dict): Mapping of unique CustomerID to zero-based indices.\n",
        "        movie_mapping (dict): Mapping of unique MovieID to zero-based indices.\n",
        "    \"\"\"\n",
        "    user_mapping = {user_id: idx for idx, user_id in enumerate(train_data['CustomerID'].unique())}\n",
        "    movie_mapping = {movie_id: idx for idx, movie_id in enumerate(train_data['MovieID'].unique())}\n",
        "    return user_mapping, movie_mapping\n",
        "\n",
        "# Generate BPR pairs using UserEncoded and MovieEncoded\n",
        "def apply_mappings(data, user_mapping, movie_mapping):\n",
        "  \"\"\"\n",
        "    Apply precomputed user and movie mappings to encode CustomerID and MovieID.\n",
        "\n",
        "    Args:\n",
        "        data (DataFrame): Dataset containing CustomerID and MovieID.\n",
        "        user_mapping (dict): Mapping for encoding users.\n",
        "        movie_mapping (dict): Mapping for encoding movies.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Dataset with UserEncoded and MovieEncoded columns added.\n",
        "    \"\"\"\n",
        "    data['UserEncoded'] = data['CustomerID'].map(user_mapping)\n",
        "    data['MovieEncoded'] = data['MovieID'].map(movie_mapping)\n",
        "    return data\n",
        "\n",
        "# Generate BPR pairs using UserEncoded and MovieEncoded\n",
        "def generate_bpr_pairs(data, y_interactions):\n",
        "  \"\"\"\n",
        "    Generate pairs for Bayesian Personalized Ranking (BPR).\n",
        "    Each pair consists of a user, a positive item (interacted with), and a negative item (not interacted with).\n",
        "\n",
        "    Args:\n",
        "        data (DataFrame): Dataset containing UserEncoded and MovieEncoded.\n",
        "        y_interactions (Series): Interaction labels (1 for positive, -1 for negative).\n",
        "\n",
        "    Returns:\n",
        "        List of tuples: Each tuple contains (user, positive_item, negative_item).\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    user_group = data.groupby('UserEncoded')\n",
        "\n",
        "    for user, group in user_group:\n",
        "        positive_items = group.loc[y_interactions[group.index] == 1, 'MovieEncoded']\n",
        "        negative_items = group.loc[y_interactions[group.index] == -1, 'MovieEncoded']\n",
        "\n",
        "        for pos_item in positive_items:\n",
        "            if not negative_items.empty:\n",
        "                neg_item = random.choice(negative_items.tolist())\n",
        "                pairs.append((user, pos_item, neg_item))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "# Define BPR model\n",
        "class BPRModel(nn.Module):\n",
        "  \"\"\"\n",
        "    BPR Model with user and item embeddings for collaborative filtering.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_users, num_items, embedding_dim):\n",
        "      \"\"\"\n",
        "        Initialize the model with user and item embeddings.\n",
        "\n",
        "        Args:\n",
        "            num_users (int): Number of unique users.\n",
        "            num_items (int): Number of unique items.\n",
        "            embedding_dim (int): Dimensionality of the embedding vectors.\n",
        "        \"\"\"\n",
        "        super(BPRModel, self).__init__()\n",
        "        self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "      \"\"\"\n",
        "        Initialize embedding weights with a normal distribution for stability in training.\n",
        "        \"\"\"\n",
        "        nn.init.normal_(self.user_embeddings.weight, mean=0.0, std=0.01)\n",
        "        nn.init.normal_(self.item_embeddings.weight, mean=0.0, std=0.01)\n",
        "\n",
        "    def forward(self, user, pos_item, neg_item):\n",
        "      \"\"\"\n",
        "        Forward pass to compute scores for positive and negative items.\n",
        "\n",
        "        Args:\n",
        "            user (Tensor): Encoded user indices.\n",
        "            pos_item (Tensor): Encoded positive item indices.\n",
        "            neg_item (Tensor): Encoded negative item indices.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Positive item scores.\n",
        "            Tensor: Negative item scores.\n",
        "        \"\"\"\n",
        "        user_emb = self.user_embeddings(user)\n",
        "        pos_item_emb = self.item_embeddings(pos_item)\n",
        "        neg_item_emb = self.item_embeddings(neg_item)\n",
        "\n",
        "        pos_scores = (user_emb * pos_item_emb).sum(dim=1)\n",
        "        neg_scores = (user_emb * neg_item_emb).sum(dim=1)\n",
        "\n",
        "        return pos_scores, neg_scores\n",
        "\n",
        "# Train BPR model with early stopping\n",
        "def BPR(model, train_pairs, val_pairs, num_epochs, learning_rate, batch_size, patience=3):\n",
        "  \"\"\"\n",
        "    Train the BPR model using SGD with early stopping based on validation loss.\n",
        "\n",
        "    Args:\n",
        "        model (BPRModel): BPR model to train.\n",
        "        train_pairs (list): Training pairs of (user, positive_item, negative_item).\n",
        "        val_pairs (list): Validation pairs for loss evaluation.\n",
        "        num_epochs (int): Maximum number of training epochs.\n",
        "        learning_rate (float): Learning rate for Adam optimizer.\n",
        "        batch_size (int): Size of mini-batches for training.\n",
        "        patience (int): Number of epochs to wait for improvement before stopping early.\n",
        "    \"\"\"\n",
        "    opt = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    ep_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      # Training loop with mini-batches\n",
        "        random.shuffle(train_pairs)\n",
        "        losses = []\n",
        "\n",
        "        for i in range(0, len(train_pairs), batch_size):\n",
        "            batch = train_pairs[i:i + batch_size]\n",
        "            users, pos_items, neg_items = zip(*batch)\n",
        "            users = torch.tensor(users, dtype=torch.long)\n",
        "            pos_items = torch.tensor(pos_items, dtype=torch.long)\n",
        "            neg_items = torch.tensor(neg_items, dtype=torch.long)\n",
        "\n",
        "            pos_scores, neg_scores = model(users, pos_items, neg_items)\n",
        "            # BCE targets: 1 for positive, 0 for negative\n",
        "\n",
        "            targets = torch.cat([torch.ones(pos_scores.size(0)), torch.zeros(neg_scores.size(0))]).to(pos_scores.device)\n",
        "            logits = torch.cat([pos_scores, neg_scores])\n",
        "\n",
        "            loss = crt(logits, targets)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            opt.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        avg_loss = np.mean(losses)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation loss evaluation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        for user, pos_item, neg_item in val_pairs:\n",
        "            user = torch.tensor([user], dtype=torch.long)\n",
        "            pos_item = torch.tensor([pos_item], dtype=torch.long)\n",
        "            neg_item = torch.tensor([neg_item], dtype=torch.long)\n",
        "\n",
        "            pos_score, neg_score = model(user, pos_item, neg_item)\n",
        "\n",
        "            logits = torch.cat([pos_score, neg_score])\n",
        "            targets = torch.cat([torch.ones(pos_score.size(0)), torch.zeros(neg_score.size(0))]).to(pos_score.device)\n",
        "            val_loss += crt(logits, targets).item()\n",
        "\n",
        "        val_loss /= len(val_pairs)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Early stopping mechanism\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            ep_no_improve = 0\n",
        "        else:\n",
        "            ep_no_improve += 1\n",
        "\n",
        "        if ep_no_improve >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_bpr(model, val_pairs, k=10):\n",
        "  \"\"\"\n",
        "    Evaluate the model using Precision@k by comparing scores of positive and negative items.\n",
        "\n",
        "    Args:\n",
        "        model (BPRModel): Trained BPR model.\n",
        "        val_pairs (list): Validation or test pairs.\n",
        "        k (int): Top-k ranking precision.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the precision score.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    hit = 0\n",
        "\n",
        "    for user, pos_item, neg_item in val_pairs:\n",
        "        user = torch.tensor([user], dtype=torch.long)\n",
        "        pos_item = torch.tensor([pos_item], dtype=torch.long)\n",
        "        neg_item = torch.tensor([neg_item], dtype=torch.long)\n",
        "\n",
        "        pos_score, neg_score = model(user, pos_item, neg_item)\n",
        "\n",
        "        if pos_score > neg_score:\n",
        "            hit += 1\n",
        "\n",
        "    accuracy = hit / len(val_pairs)\n",
        "    print(f\"Precision@{k}: {accuracy:.4f}\")\n",
        "\n",
        "# Main script\n",
        "# Create user and movie mappings\n",
        "user_mapping, movie_mapping = Mapping(train_data_bpr)\n",
        "# Apply mappings to training, validation, and test data\n",
        "train_data_bpr = apply_mappings(train_data_bpr, user_mapping, movie_mapping)\n",
        "val_data_bpr = apply_mappings(val_data_bpr, user_mapping, movie_mapping)\n",
        "test_data_bpr = apply_mappings(test_data_bpr, user_mapping, movie_mapping)\n",
        "\n",
        "# Generate training, validation, and test pairs\n",
        "train_pairs = generate_bpr_pairs(train_data_bpr, y_train_interactions)\n",
        "val_pairs = generate_bpr_pairs(val_data_bpr, y_val_interactions)\n",
        "test_pairs = generate_bpr_pairs(test_data_bpr, y_test_interactions)\n",
        "\n",
        "print(f\"Generated {len(train_pairs)} training pairs.\")\n",
        "print(f\"Generated {len(val_pairs)} validation pairs.\")\n",
        "print(f\"Generated {len(test_pairs)} test pairs.\")\n",
        "\n",
        "# Define model parameters\n",
        "num_users = train_data_bpr['UserEncoded'].max() + 1\n",
        "num_items = train_data_bpr['MovieEncoded'].max() + 1\n",
        "embedding_dim = 50\n",
        "\n",
        "# Instantiate and train the model\n",
        "bpr_model = BPRModel(num_users, num_items, embedding_dim)\n",
        "\n",
        "BPR(bpr_model, train_pairs, val_pairs, num_epochs=10, learning_rate=0.01, batch_size=1024, patience=3)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_bpr(bpr_model, val_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAAG_MThhP4h",
        "outputId": "e10dab89-62bb-4f41-caca-516325c33eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 17988776 training pairs.\n",
            "Generated 1865476 validation pairs.\n",
            "Generated 1870576 test pairs.\n",
            "Epoch 1/10, Training Loss: 0.3991\n",
            "Epoch 1/10, Validation Loss: 0.6955\n",
            "Epoch 2/10, Training Loss: 0.2522\n",
            "Epoch 2/10, Validation Loss: 0.9562\n",
            "Epoch 3/10, Training Loss: 0.2108\n",
            "Epoch 3/10, Validation Loss: 1.2215\n",
            "Epoch 4/10, Training Loss: 0.1915\n",
            "Epoch 4/10, Validation Loss: 1.4689\n",
            "Early stopping triggered at epoch 4\n",
            "Precision@10: 0.8345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate pdf\n",
        "# Please provide the full path of the notebook file below\n",
        "# Important: make sure that your file name does not contain spaces!\n",
        "\n",
        "notebookpath = '/content/drive/My Drive/Colab Notebooks/dev_without_xgboost.ipynb'\n",
        "drive_mount_point = '/content/drive/'\n",
        "from google.colab import drive\n",
        "drive.mount(drive_mount_point)\n",
        "file_name = notebookpath.split('/')[-1]\n",
        "get_ipython().system(\"apt update && apt install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\")\n",
        "get_ipython().system(\"pip install pypandoc\")\n",
        "get_ipython().system(\"apt-get install texlive texlive-xetex texlive-latex-extra pandoc\")\n",
        "get_ipython().system(\"jupyter nbconvert --to PDF {}\".format(notebookpath.replace(' ', '\\\\ ')))\n",
        "from google.colab import files\n",
        "files.download(notebookpath.split('.')[0]+'.pdf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YvFZEB9P7hxb",
        "outputId": "d392b966-dc52-480b-b848-cb33785cd7dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,192 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,454 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,626 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,738 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,531 kB]\n",
            "Fetched 18.0 MB in 4s (4,307 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "52 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "\u001b[1;31mE: \u001b[0mUnable to locate package texlive-generic-recommended\u001b[0m\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.14-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.14-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.14\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  fonts-urw-base35 libapache-pom-java libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3\n",
            "  libcommons-logging-java libcommons-parent-java libfontbox-java libfontenc1 libgs9 libgs9-common\n",
            "  libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0 libsynctex2\n",
            "  libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13 lmodern pandoc-data poppler-data\n",
            "  preview-latex-style rake ruby ruby-net-telnet ruby-rubygems ruby-webrick ruby-xmlrpc ruby3.0\n",
            "  rubygems-integration t1utils teckit tex-common tex-gyre texlive-base texlive-binaries\n",
            "  texlive-fonts-recommended texlive-latex-base texlive-latex-recommended texlive-pictures\n",
            "  texlive-plain-generic tipa xfonts-encodings xfonts-utils\n",
            "Suggested packages:\n",
            "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf libavalon-framework-java\n",
            "  libcommons-logging-java-doc libexcalibur-logkit-java liblog4j1.2-java texlive-luatex\n",
            "  pandoc-citeproc context wkhtmltopdf librsvg2-bin groff ghc nodejs php python libjs-mathjax\n",
            "  libjs-katex citation-style-language-styles poppler-utils ghostscript fonts-japanese-mincho\n",
            "  | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
            "  fonts-arphic-uming fonts-nanum ri ruby-dev bundler debhelper gv | postscript-viewer perl-tk xpdf\n",
            "  | pdf-viewer xzdec texlive-fonts-recommended-doc texlive-latex-base-doc python3-pygments\n",
            "  icc-profiles libfile-which-perl libspreadsheet-parseexcel-perl texlive-latex-extra-doc\n",
            "  texlive-latex-recommended-doc texlive-pstricks dot2tex prerex texlive-pictures-doc vprerex\n",
            "  default-jre-headless tipa-doc\n",
            "The following NEW packages will be installed:\n",
            "  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  fonts-urw-base35 libapache-pom-java libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3\n",
            "  libcommons-logging-java libcommons-parent-java libfontbox-java libfontenc1 libgs9 libgs9-common\n",
            "  libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0 libsynctex2\n",
            "  libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13 lmodern pandoc pandoc-data\n",
            "  poppler-data preview-latex-style rake ruby ruby-net-telnet ruby-rubygems ruby-webrick ruby-xmlrpc\n",
            "  ruby3.0 rubygems-integration t1utils teckit tex-common tex-gyre texlive texlive-base\n",
            "  texlive-binaries texlive-fonts-recommended texlive-latex-base texlive-latex-extra\n",
            "  texlive-latex-recommended texlive-pictures texlive-plain-generic texlive-xetex tipa\n",
            "  xfonts-encodings xfonts-utils\n",
            "0 upgraded, 59 newly installed, 0 to remove and 52 not upgraded.\n",
            "Need to get 202 MB of archives.\n",
            "After this operation, 728 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lato all 2.0-2.1 [2,696 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-common all 6.17 [33.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.10 [752 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.10 [5,031 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.2 [60.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dvisvgm amd64 2.13.1-1 [1,221 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lmodern all 2.004.5-6.1 [4,532 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-texgyre all 20180621-3.1 [10.2 MB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libapache-pom-java all 18-1 [4,720 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-parent-java all 43-1 [10.8 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-logging-java all 1.2-2 [60.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libptexenc1 amd64 2021.20210626.59705-1ubuntu0.2 [39.1 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 rubygems-integration all 1.18 [5,336 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby3.0 amd64 3.0.2-7ubuntu2.8 [50.1 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-rubygems all 3.3.5-2 [228 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby amd64 1:3.0~exp1 [5,100 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 rake all 13.0.6-2 [61.7 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-webrick all 1.7.0-3ubuntu0.1 [52.1 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-xmlrpc all 0.3.2-1ubuntu0.1 [24.9 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libruby3.0 amd64 3.0.2-7ubuntu2.8 [5,113 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.2 [55.6 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libteckit0 amd64 2.5.11+ds1-1 [421 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexlua53 amd64 2021.20210626.59705-1ubuntu0.2 [120 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexluajit2 amd64 2021.20210626.59705-1ubuntu0.2 [267 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzzip-0-13 amd64 0.13.72+dfsg.1-1.1 [27.0 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmodern all 2.004.5-6.1 [9,471 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 preview-latex-style all 12.2-1ubuntu1 [185 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 t1utils amd64 1.41-4build2 [61.3 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 teckit amd64 2.5.11+ds1-1 [699 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-gyre all 20180621-3.1 [6,209 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 texlive-binaries amd64 2021.20210626.59705-1ubuntu0.2 [9,860 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-base all 2021.20220204-1 [21.0 MB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-recommended all 2021.20220204-1 [4,972 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-base all 2021.20220204-1 [1,128 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-recommended all 2021.20220204-1 [14.4 MB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive all 2021.20220204-1 [14.3 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfontbox-java all 1:1.8.16-2 [207 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpdfbox-java all 1:1.8.16-2 [5,199 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-pictures all 2021.20220204-1 [8,720 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-extra all 2021.20220204-1 [13.9 MB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-plain-generic all 2021.20220204-1 [27.5 MB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tipa all 2:1.3-21 [2,967 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-xetex all 2021.20220204-1 [12.4 MB]\n",
            "Fetched 202 MB in 10s (20.1 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2.1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.17_all.deb ...\n",
            "Unpacking tex-common (6.17) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.10_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.10_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libwoff1:amd64.\n",
            "Preparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...\n",
            "Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Selecting previously unselected package dvisvgm.\n",
            "Preparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...\n",
            "Unpacking dvisvgm (2.13.1-1) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../13-fonts-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../14-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../15-fonts-texgyre_20180621-3.1_all.deb ...\n",
            "Unpacking fonts-texgyre (20180621-3.1) ...\n",
            "Selecting previously unselected package libapache-pom-java.\n",
            "Preparing to unpack .../16-libapache-pom-java_18-1_all.deb ...\n",
            "Unpacking libapache-pom-java (18-1) ...\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../17-libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../18-libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcommons-parent-java.\n",
            "Preparing to unpack .../19-libcommons-parent-java_43-1_all.deb ...\n",
            "Unpacking libcommons-parent-java (43-1) ...\n",
            "Selecting previously unselected package libcommons-logging-java.\n",
            "Preparing to unpack .../20-libcommons-logging-java_1.2-2_all.deb ...\n",
            "Unpacking libcommons-logging-java (1.2-2) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../21-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../22-libptexenc1_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../23-rubygems-integration_1.18_all.deb ...\n",
            "Unpacking rubygems-integration (1.18) ...\n",
            "Selecting previously unselected package ruby3.0.\n",
            "Preparing to unpack .../24-ruby3.0_3.0.2-7ubuntu2.8_amd64.deb ...\n",
            "Unpacking ruby3.0 (3.0.2-7ubuntu2.8) ...\n",
            "Selecting previously unselected package ruby-rubygems.\n",
            "Preparing to unpack .../25-ruby-rubygems_3.3.5-2_all.deb ...\n",
            "Unpacking ruby-rubygems (3.3.5-2) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../26-ruby_1%3a3.0~exp1_amd64.deb ...\n",
            "Unpacking ruby (1:3.0~exp1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../27-rake_13.0.6-2_all.deb ...\n",
            "Unpacking rake (13.0.6-2) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../28-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-webrick.\n",
            "Preparing to unpack .../29-ruby-webrick_1.7.0-3ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-xmlrpc.\n",
            "Preparing to unpack .../30-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libruby3.0:amd64.\n",
            "Preparing to unpack .../31-libruby3.0_3.0.2-7ubuntu2.8_amd64.deb ...\n",
            "Unpacking libruby3.0:amd64 (3.0.2-7ubuntu2.8) ...\n",
            "Selecting previously unselected package libsynctex2:amd64.\n",
            "Preparing to unpack .../32-libsynctex2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libteckit0:amd64.\n",
            "Preparing to unpack .../33-libteckit0_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package libtexlua53:amd64.\n",
            "Preparing to unpack .../34-libtexlua53_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../35-libtexluajit2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../36-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../37-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../38-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../39-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../40-pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../41-pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../42-preview-latex-style_12.2-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (12.2-1ubuntu1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../43-t1utils_1.41-4build2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-4build2) ...\n",
            "Selecting previously unselected package teckit.\n",
            "Preparing to unpack .../44-teckit_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking teckit (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../45-tex-gyre_20180621-3.1_all.deb ...\n",
            "Unpacking tex-gyre (20180621-3.1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../46-texlive-binaries_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../47-texlive-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../48-texlive-fonts-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../49-texlive-latex-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../50-texlive-latex-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive.\n",
            "Preparing to unpack .../51-texlive_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive (2021.20220204-1) ...\n",
            "Selecting previously unselected package libfontbox-java.\n",
            "Preparing to unpack .../52-libfontbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libfontbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package libpdfbox-java.\n",
            "Preparing to unpack .../53-libpdfbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libpdfbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../54-texlive-pictures_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-pictures (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../55-texlive-latex-extra_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-extra (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../56-texlive-plain-generic_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-plain-generic (2021.20220204-1) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../57-tipa_2%3a1.3-21_all.deb ...\n",
            "Unpacking tipa (2:1.3-21) ...\n",
            "Selecting previously unselected package texlive-xetex.\n",
            "Preparing to unpack .../58-texlive-xetex_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-xetex (2021.20220204-1) ...\n",
            "Setting up fonts-lato (2.0-2.1) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Setting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libfontbox-java (1:1.8.16-2) ...\n",
            "Setting up rubygems-integration (1.18) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up tex-common (6.17) ...\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Setting up libapache-pom-java (18-1) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up t1utils (1.41-4build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up fonts-texgyre (20180621-3.1) ...\n",
            "Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up fonts-lmodern (2.004.5-6.1) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Setting up teckit (2.5.11+ds1-1) ...\n",
            "Setting up libpdfbox-java (1:1.8.16-2) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Setting up preview-latex-style (12.2-1ubuntu1) ...\n",
            "Setting up libcommons-parent-java (43-1) ...\n",
            "Setting up dvisvgm (2.13.1-1) ...\n",
            "Setting up libcommons-logging-java (1.2-2) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up lmodern (2.004.5-6.1) ...\n",
            "Setting up texlive-base (2021.20220204-1) ...\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex\n",
            "Setting up tex-gyre (20180621-3.1) ...\n",
            "Setting up texlive-plain-generic (2021.20220204-1) ...\n",
            "Setting up texlive-latex-base (2021.20220204-1) ...\n",
            "Setting up texlive-latex-recommended (2021.20220204-1) ...\n",
            "Setting up texlive-pictures (2021.20220204-1) ...\n",
            "Setting up texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Setting up tipa (2:1.3-21) ...\n",
            "Setting up texlive (2021.20220204-1) ...\n",
            "Setting up texlive-latex-extra (2021.20220204-1) ...\n",
            "Setting up texlive-xetex (2021.20220204-1) ...\n",
            "Setting up rake (13.0.6-2) ...\n",
            "Setting up libruby3.0:amd64 (3.0.2-7ubuntu2.8) ...\n",
            "Setting up ruby3.0 (3.0.2-7ubuntu2.8) ...\n",
            "Setting up ruby (1:3.0~exp1) ...\n",
            "Setting up ruby-rubygems (3.3.5-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for tex-common (6.17) ...\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "[NbConvertApp] Converting notebook /content/drive/My Drive/Colab Notebooks/dev_without_xgboost.ipynb to PDF\n",
            "[NbConvertApp] Support files will be in dev_without_xgboost_files/\n",
            "[NbConvertApp] Making directory ./dev_without_xgboost_files\n",
            "[NbConvertApp] Writing 183459 bytes to notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 175250 bytes to /content/drive/My Drive/Colab Notebooks/dev_without_xgboost.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f04a451d-bbea-498b-95cc-d0d002ca163c\", \"dev_without_xgboost.pdf\", 175250)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
